#!/usr/bin/env python3
"""
–ë–µ–Ω—á–º–∞—Ä–∫ ULTRA BOT - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
===============================================================

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –Ω–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ò–ò –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""

import time
import torch
import numpy as np
import cv2
import psutil
import GPUtil
from ultralytics import YOLO
import json
from pathlib import Path
import matplotlib.pyplot as plt
from datetime import datetime

class UltraBotBenchmark:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ULTRA BOT"""

    def __init__(self):
        self.results = {}
        self.system_info = self.get_system_info()

    def get_system_info(self):
        """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        info = {
            'cpu': psutil.cpu_count(),
            'cpu_logical': psutil.cpu_count(logical=True),
            'ram_total': psutil.virtual_memory().total / (1024**3),  # GB
            'ram_available': psutil.virtual_memory().available / (1024**3),  # GB
        }

        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                info.update({
                    'gpu_name': gpu.name,
                    'gpu_memory_total': gpu.memoryTotal,
                    'gpu_memory_free': gpu.memoryFree,
                    'gpu_driver': gpu.driver,
                    'gpu_cuda': torch.cuda.is_available()
                })
        except:
            info['gpu_name'] = 'N/A'
            info['gpu_cuda'] = False

        return info

    def benchmark_inference_speed(self, model_path, sizes=[256, 320, 416, 512, 640], iterations=50):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
        print(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {Path(model_path).name}")

        results = {}

        try:
            model = YOLO(model_path)
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            model.to(device)

            for size in sizes:
                print(f"  üìè –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ {size}px...")

                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –±–∞—Ç—á–∞
                batch_size = 1
                dummy_input = torch.randn(batch_size, 3, size, size).to(device)

                # –†–∞–∑–æ–≥—Ä–µ–≤
                for _ in range(5):
                    _ = model(dummy_input, verbose=False)

                # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç
                torch.cuda.synchronize() if device == 'cuda' else None
                start_time = time.time()

                for _ in range(iterations):
                    _ = model(dummy_input, verbose=False)

                torch.cuda.synchronize() if device == 'cuda' else None
                end_time = time.time()

                total_time = end_time - start_time
                fps = iterations / total_time
                latency = (total_time / iterations) * 1000  # ms

                results[size] = {
                    'fps': round(fps, 2),
                    'latency_ms': round(latency, 2),
                    'total_time': round(total_time, 3)
                }

                print(f"    ‚úÖ {fps:.1f} FPS, {latency:.1f}ms latency")

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            results = {'error': str(e)}

        return results

    def benchmark_accuracy(self, model_path, test_images=None):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        print(f"üéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: {Path(model_path).name}")

        if not test_images:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            test_images = self.generate_synthetic_test_images()

        results = {
            'total_images': len(test_images),
            'detections': 0,
            'avg_confidence': 0,
            'false_positives': 0,
            'false_negatives': 0
        }

        try:
            model = YOLO(model_path)
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            model.to(device)

            total_conf = 0

            for img_path in test_images:
                results_img = model(img_path, conf=0.1, verbose=False)

                if results_img and len(results_img) > 0:
                    boxes = results_img[0].boxes
                    if boxes is not None:
                        confs = boxes.conf.cpu().numpy()
                        results['detections'] += len(confs)
                        total_conf += np.sum(confs)

            if results['detections'] > 0:
                results['avg_confidence'] = total_conf / results['detections']

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏: {e}")
            results['error'] = str(e)

        return results

    def generate_synthetic_test_images(self, count=10):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        images = []

        for i in range(count):
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∫—Ä—É–≥–∞–º–∏ (–∏–º–∏—Ç–∞—Ü–∏—è –º—è—á–∏–∫–æ–≤)
            img = np.zeros((416, 416, 3), dtype=np.uint8)
            img.fill(100)  # –°–µ—Ä—ã–π —Ñ–æ–Ω

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –∫—Ä—É–≥–æ–≤
            for _ in range(np.random.randint(1, 5)):
                center = (np.random.randint(50, 366), np.random.randint(50, 366))
                radius = np.random.randint(10, 30)
                color = (np.random.randint(200, 255), np.random.randint(100, 200), np.random.randint(50, 150))
                cv2.circle(img, center, radius, color, -1)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            img_path = f"temp_test_{i}.png"
            cv2.imwrite(img_path, img)
            images.append(img_path)

        return images

    def benchmark_memory_usage(self, model_path):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        print(f"üíæ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {Path(model_path).name}")

        results = {}

        try:
            # –ó–∞–º–µ—Ä –±–∞–∑–æ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            process = psutil.Process()
            base_memory = process.memory_info().rss / (1024**2)  # MB

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            model = YOLO(model_path)
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            model.to(device)

            # –ó–∞–º–µ—Ä –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            load_memory = process.memory_info().rss / (1024**2)  # MB

            # –¢–µ—Å—Ç–æ–≤—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
            dummy_input = torch.randn(1, 3, 416, 416).to(device)
            _ = model(dummy_input, verbose=False)

            # –ó–∞–º–µ—Ä –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
            work_memory = process.memory_info().rss / (1024**2)  # MB

            # GPU –ø–∞–º—è—Ç—å
            gpu_memory = 0
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB

            results = {
                'base_memory_mb': round(base_memory, 1),
                'load_memory_mb': round(load_memory, 1),
                'work_memory_mb': round(work_memory, 1),
                'model_memory_delta_mb': round(load_memory - base_memory, 1),
                'gpu_memory_mb': round(gpu_memory, 1)
            }

            print(f"    üìä RAM: {results['model_memory_delta_mb']}MB, GPU: {results['gpu_memory_mb']}MB")

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏: {e}")
            results = {'error': str(e)}

        return results

    def run_full_benchmark(self, models=None, output_file="benchmark_results.json"):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
        print("üöÄ ULTRA BOT BENCHMARK v6.0")
        print("=" * 50)
        print(f"üíª –°–∏—Å—Ç–µ–º–∞: {self.system_info}")
        print()

        if not models:
            # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            weights_dir = Path("weights")
            models = []
            if weights_dir.exists():
                for pt_file in weights_dir.glob("*.pt"):
                    if "ball" in pt_file.name.lower() or "cand" in pt_file.name.lower():
                        models.append(str(pt_file))

        if not models:
            print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ weights/")
            return

        print(f"üéØ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")
        for model in models:
            print(f"  - {Path(model).name}")
        print()

        all_results = {
            'timestamp': datetime.now().isoformat(),
            'system_info': self.system_info,
            'models': {}
        }

        for model_path in models:
            model_name = Path(model_path).name
            print(f"üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
            print("-" * 40)

            model_results = {}

            # 1. –°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            model_results['inference_speed'] = self.benchmark_inference_speed(model_path)

            # 2. –¢–æ—á–Ω–æ—Å—Ç—å
            model_results['accuracy'] = self.benchmark_accuracy(model_path)

            # 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
            model_results['memory'] = self.benchmark_memory_usage(model_path)

            all_results['models'][model_name] = model_results
            print()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)

        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")

        # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
        self.print_summary(all_results)

        return all_results

    def print_summary(self, results):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("\\nüìä –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("=" * 50)

        for model_name, model_data in results['models'].items():
            print(f"\\nüéØ {model_name}:")

            if 'inference_speed' in model_data and 'error' not in model_data['inference_speed']:
                speeds = model_data['inference_speed']
                best_size = max(speeds.keys(), key=lambda x: speeds[x]['fps'] if isinstance(speeds[x], dict) else 0)
                best_fps = speeds[best_size]['fps'] if isinstance(speeds[best_size], dict) else 0
                print(f"  üöÄ –õ—É—á—à–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: {best_fps} FPS –ø—Ä–∏ {best_size}px")

            if 'accuracy' in model_data and 'error' not in model_data['accuracy']:
                acc = model_data['accuracy']
                print(f"  üéØ –î–µ—Ç–µ–∫—Ü–∏–π: {acc['detections']}, –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {acc.get('avg_confidence', 0):.3f}")

            if 'memory' in model_data and 'error' not in model_data['memory']:
                mem = model_data['memory']
                print(f"  üíæ –ü–∞–º—è—Ç—å: +{mem['model_memory_delta_mb']}MB RAM")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        print("\\nüéñÔ∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:")
        best_model = None
        best_score = 0

        for model_name, model_data in results['models'].items():
            score = 0
            if 'inference_speed' in model_data:
                speeds = model_data['inference_speed']
                if not isinstance(speeds, dict) or 'error' in speeds:
                    continue
                max_fps = max(s['fps'] for s in speeds.values() if isinstance(s, dict))
                score += max_fps * 0.6

            if 'accuracy' in model_data:
                acc = model_data['accuracy']
                if isinstance(acc, dict) and 'detections' in acc:
                    score += acc['detections'] * 0.4

            if score > best_score:
                best_score = score
                best_model = model_name

        if best_model:
            print(f"  üèÜ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –º–æ–¥–µ–ª—å: {best_model}")
        else:
            print("  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å")

def main():
    import argparse

    parser = argparse.ArgumentParser(description='ULTRA BOT Benchmark Suite')
    parser.add_argument('--models', nargs='+', help='–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--output', default='benchmark_results.json', help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--quick', action='store_true', help='–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (–º–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π)')

    args = parser.parse_args()

    benchmark = UltraBotBenchmark()

    if args.quick:
        # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
        print("‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        iterations = 10
        sizes = [320, 416]
    else:
        iterations = 50
        sizes = [256, 320, 416, 512, 640]

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∫–ª–∞—Å—Å–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞
    original_benchmark = benchmark.benchmark_inference_speed
    def quick_benchmark(model_path, sizes=sizes, iterations=iterations):
        return original_benchmark(model_path, sizes, iterations)
    benchmark.benchmark_inference_speed = quick_benchmark

    try:
        results = benchmark.run_full_benchmark(args.models, args.output)
        print("\\n‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")

    except KeyboardInterrupt:
        print("\\n‚èπÔ∏è –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\\n‚ùå –û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")

if __name__ == "__main__":
    main()